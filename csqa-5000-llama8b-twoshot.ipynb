{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13000706,"sourceType":"datasetVersion","datasetId":8229833},{"sourceId":13035549,"sourceType":"datasetVersion","datasetId":8254032}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T23:55:57.275011Z","iopub.execute_input":"2025-10-30T23:55:57.275277Z","iopub.status.idle":"2025-10-30T23:55:57.555231Z","shell.execute_reply.started":"2025-10-30T23:55:57.275248Z","shell.execute_reply":"2025-10-30T23:55:57.554441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load JSON\nwith open(\"/kaggle/input/csqa-logicalcombinations/logical_combinations_output.json\", \"r\") as f:\n    data = json.load(f)\n\n# Convert the 'questions' list to a DataFrame\ndf = pd.DataFrame(data[\"questions\"])\ndf.to_csv('logical_combinations_output.csv',index=False)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T23:55:58.383844Z","iopub.execute_input":"2025-10-30T23:55:58.384667Z","iopub.status.idle":"2025-10-30T23:55:59.363281Z","shell.execute_reply.started":"2025-10-30T23:55:58.384639Z","shell.execute_reply":"2025-10-30T23:55:59.362488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes peft trl datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:02:39.800154Z","iopub.execute_input":"2025-10-31T00:02:39.800452Z","iopub.status.idle":"2025-10-31T00:02:43.283111Z","shell.execute_reply.started":"2025-10-31T00:02:39.800424Z","shell.execute_reply":"2025-10-31T00:02:43.282335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"hugging_face\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:03:02.835392Z","iopub.execute_input":"2025-10-31T00:03:02.835765Z","iopub.status.idle":"2025-10-31T00:03:02.924215Z","shell.execute_reply.started":"2025-10-31T00:03:02.835741Z","shell.execute_reply":"2025-10-31T00:03:02.923700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:03:03.493247Z","iopub.execute_input":"2025-10-31T00:03:03.493499Z","iopub.status.idle":"2025-10-31T00:03:03.793115Z","shell.execute_reply.started":"2025-10-31T00:03:03.493482Z","shell.execute_reply":"2025-10-31T00:03:03.792482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import random\n# import ast\n\n# # Load your dataset\n# df = pd.read_csv(\"/kaggle/working/logical_combinations_output.csv\")  # replace with your file\n\n# # Safe eval to ensure lists are properly loaded\n# def safe_eval(val):\n#     if isinstance(val, list):\n#         return val\n#     if pd.isna(val):\n#         return []\n#     if isinstance(val, str):\n#         try:\n#             return ast.literal_eval(val)\n#         except:\n#             return []\n#     return []\n\n# df['logical_combinations'] = df['logical_combinations'].apply(safe_eval)\n\n# # Prepare storage for QA sets\n# qa_sets = {\n#     'AND': [],\n#     'OR': [],\n#     'NEITHER': [],\n#     'Mixed': []\n# }\n\n# # Label rotation - separate rotator for each question type\n# class LabelRotator:\n#     def __init__(self, name):\n#         self.name = name\n#         self.labels = ['A', 'B', 'C', 'D']\n#         self.current_index = 0\n    \n#     def get_next_label(self):\n#         label = self.labels[self.current_index]\n#         self.current_index = (self.current_index + 1) % len(self.labels)\n#         # print(f\"DEBUG {self.name}: Assigned label {label}, next will be {self.labels[self.current_index]}\")\n#         return label\n\n# # Create separate rotators for each question type\n# rotators = {\n#     'AND': LabelRotator('AND'),\n#     'OR': LabelRotator('OR'),\n#     'NEITHER': LabelRotator('NEITHER'),\n#     'Mixed': LabelRotator('Mixed')\n# }\n\n# def create_qa_item(question, correct_ans, incorrect_ans, qa_type):\n#     \"\"\"Helper function to create a QA item with proper label rotation\"\"\"\n    \n#     # Get correct label from the appropriate rotator\n#     correct_label = rotators[qa_type].get_next_label()\n    \n#     # Ensure we have exactly 3 incorrect answers\n#     if len(incorrect_ans) < 3:\n#         print(f\"Warning: Only {len(incorrect_ans)} incorrect answers available for {qa_type}\")\n#         return None\n    \n#     selected_incorrect = random.sample(incorrect_ans, 3)\n    \n#     # Create options array with 4 positions\n#     options = [''] * 4\n    \n#     # Place correct answer at the designated position\n#     correct_pos = rotators[qa_type].labels.index(correct_label)\n#     options[correct_pos] = correct_ans\n    \n#     # Fill remaining positions with incorrect answers\n#     incorrect_positions = [i for i in range(4) if i != correct_pos]\n#     for i, pos in enumerate(incorrect_positions):\n#         options[pos] = selected_incorrect[i]\n    \n#     return {\n#         'question': question,\n#         'A': options[0],\n#         'B': options[1],\n#         'C': options[2],\n#         'D': options[3],\n#         'correct_label': correct_label,\n#         'correct_answer_text': correct_ans,\n#         'qa_type': qa_type\n#     }\n\n# # Track counts for each type\n# type_counts = {'AND': 0, 'OR': 0, 'NEITHER': 0, 'Mixed': 0}\n\n# for idx, row in df.iterrows():\n#     question = row['question']\n#     combos = row['logical_combinations']\n    \n#     # Extract AND/OR/NEITHER correct and incorrect lists\n#     and_correct = combos.get('AND_combinations', {}).get('correct', [])\n#     and_incorrect = combos.get('AND_combinations', {}).get('incorrect', [])\n    \n#     or_correct = combos.get('OR_combinations', {}).get('correct', [])\n#     or_incorrect = combos.get('OR_combinations', {}).get('incorrect', [])\n    \n#     neither_correct = combos.get('NEITHER_combinations', {}).get('correct', [])\n#     neither_incorrect = combos.get('NEITHER_combinations', {}).get('incorrect', [])\n    \n#     # --- AND-only QA ---\n#     if and_correct and len(and_incorrect) >= 3:\n#         correct_ans = random.choice(and_correct)\n#         qa_item = create_qa_item(question, correct_ans, and_incorrect, 'AND')\n#         if qa_item:\n#             qa_sets['AND'].append(qa_item)\n#             type_counts['AND'] += 1\n#             # print(f\"AND Question {type_counts['AND']}: Label {qa_item['correct_label']}\")\n    \n#     # --- OR-only QA ---\n#     if or_correct and len(or_incorrect) >= 3:\n#         correct_ans = random.choice(or_correct)\n#         qa_item = create_qa_item(question, correct_ans, or_incorrect, 'OR')\n#         if qa_item:\n#             qa_sets['OR'].append(qa_item)\n#             type_counts['OR'] += 1\n#             # print(f\"OR Question {type_counts['OR']}: Label {qa_item['correct_label']}\")\n    \n#     # --- NEITHER-only QA ---\n#     if neither_correct and len(neither_incorrect) >= 3:\n#         correct_ans = random.choice(neither_correct)\n#         qa_item = create_qa_item(question, correct_ans, neither_incorrect, 'NEITHER')\n#         if qa_item:\n#             qa_sets['NEITHER'].append(qa_item)\n#             type_counts['NEITHER'] += 1\n#             # print(f\"NEITHER Question {type_counts['NEITHER']}: Label {qa_item['correct_label']}\")\n    \n#     # --- Mixed QA ---\n#     all_correct_types = [\n#         ('AND', and_correct),\n#         ('OR', or_correct),\n#         ('NEITHER', neither_correct)\n#     ]\n#     all_incorrect_types = and_incorrect + or_incorrect + neither_incorrect\n    \n#     # Ensure at least one correct exists and enough incorrect answers\n#     valid_categories = [cat for cat, lst in all_correct_types if lst]\n#     if valid_categories and len(all_incorrect_types) >= 3:\n#         chosen_category = random.choice(valid_categories)\n#         chosen_correct_list = dict(all_correct_types)[chosen_category]\n#         correct_ans = random.choice(chosen_correct_list)\n#         qa_item = create_qa_item(question, correct_ans, all_incorrect_types, 'Mixed')\n#         if qa_item:\n#             qa_sets['Mixed'].append(qa_item)\n#             type_counts['Mixed'] += 1\n#             # print(f\"Mixed Question {type_counts['Mixed']}: Label {qa_item['correct_label']}\")\n\n# print(f\"\\n=== GENERATION COMPLETE ===\")\n# for qtype, count in type_counts.items():\n#     print(f\"{qtype}: {count} questions generated\")\n\n# # --- Convert to DataFrames and save ---\n# for key, qlist in qa_sets.items():\n#     if qlist:  # Only save if we have questions\n#         df_out = pd.DataFrame(qlist)\n#         df_out.to_csv(f'QA_{key}.csv', index=False)\n#         print(f\"\\n{key} QA saved: {len(df_out)} questions in QA_{key}.csv\")\n        \n#         # Show label distribution for verification\n#         label_counts = df_out['correct_label'].value_counts().sort_index()\n#         print(f\"  Label distribution: {dict(label_counts)}\")\n        \n#         # Show label sequence for first 8 questions to verify rotation\n#         print(f\"  Label sequence (first 8): {list(df_out['correct_label'].head(8))}\")\n#     else:\n#         print(f\"{key}: No valid QA pairs generated\")\n\n# # Overall statistics\n# total_questions = sum(len(qlist) for qlist in qa_sets.values())\n# print(f\"\\nFinal total questions generated: {total_questions}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-30T23:54:33.667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_df = pd.read_csv('/kaggle/input/csqa-pairs-baselines/QA_AND.csv')\nqa_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:03:09.538846Z","iopub.execute_input":"2025-10-31T00:03:09.539644Z","iopub.status.idle":"2025-10-31T00:03:09.603926Z","shell.execute_reply.started":"2025-10-31T00:03:09.539612Z","shell.execute_reply":"2025-10-31T00:03:09.603073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef calculate_qa_metrics(results_df):\n    \"\"\"\n    \n    Calculate comprehensive evaluation metrics for QA results\n    \n    Args:\n        results_df: DataFrame with 'predicted_label' and 'correct_label' columns\n    \n    Returns:\n        dict: Dictionary containing all evaluation metrics\n    \"\"\"\n    \n    # Remove None predictions for accurate calculation\n    valid_predictions = results_df.dropna(subset=['predicted_label'])\n    \n    if len(valid_predictions) == 0:\n        print(\"Warning: No valid predictions found!\")\n        return {}\n    \n    y_true = valid_predictions['correct_label'].tolist()\n    y_pred = valid_predictions['predicted_label'].tolist()\n    \n    # Basic metrics\n    total_questions = len(results_df)\n    answered_questions = len(valid_predictions)\n    unanswered_questions = total_questions - answered_questions\n    \n    # Accuracy (main metric for QA)\n    accuracy = accuracy_score(y_true, y_pred)\n    \n    # Per-class metrics\n    labels = ['A', 'B', 'C', 'D']\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n    \n    # Macro and micro averages\n    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='macro', zero_division=0\n    )\n    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='micro', zero_division=0\n    )\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    \n    # Answer rate (percentage of questions answered)\n    answer_rate = answered_questions / total_questions\n    \n    # Compile results\n    metrics = {\n        'total_questions': total_questions,\n        'answered_questions': answered_questions,\n        'unanswered_questions': unanswered_questions,\n        'answer_rate': answer_rate,\n        'accuracy': accuracy,\n        'macro_precision': macro_precision,\n        'macro_recall': macro_recall,\n        'macro_f1': macro_f1,\n        'micro_precision': micro_precision,\n        'micro_recall': micro_recall,\n        'micro_f1': micro_f1,\n    }\n    \n    # Per-class metrics\n    for i, label in enumerate(labels):\n        metrics[f'{label}_precision'] = precision[i]\n        metrics[f'{label}_recall'] = recall[i]\n        metrics[f'{label}_f1'] = f1[i]\n        metrics[f'{label}_support'] = support[i]\n    \n    return metrics, cm, y_true, y_pred\n\ndef print_qa_metrics(metrics):\n    \"\"\"Pretty print the evaluation metrics\"\"\"\n    print(\"=\" * 50)\n    print(\"QA EVALUATION METRICS\")\n    print(\"=\" * 50)\n    \n    print(f\"Total Questions: {metrics['total_questions']}\")\n    print(f\"Answered Questions: {metrics['answered_questions']}\")\n    print(f\"Answer Rate: {metrics['answer_rate']:.1%}\")\n    print(f\"Accuracy: {metrics['accuracy']:.1%}\")\n    \n    print(\"\\n\" + \"-\" * 30)\n    print(\"OVERALL METRICS\")\n    print(\"-\" * 30)\n    print(f\"Macro Precision: {metrics['macro_precision']:.3f}\")\n    print(f\"Macro Recall: {metrics['macro_recall']:.3f}\")\n    print(f\"Macro F1: {metrics['macro_f1']:.3f}\")\n    print(f\"Micro Precision: {metrics['micro_precision']:.3f}\")\n    print(f\"Micro Recall: {metrics['micro_recall']:.3f}\")\n    print(f\"Micro F1: {metrics['micro_f1']:.3f}\")\n    \n    print(\"\\n\" + \"-\" * 40)\n    print(\"PER-CLASS METRICS\")\n    print(\"-\" * 40)\n    print(f\"{'Class':<5} {'Precision':<9} {'Recall':<6} {'F1':<6} {'Support'}\")\n    print(\"-\" * 40)\n    \n    for label in ['A', 'B', 'C', 'D']:\n        precision = metrics[f'{label}_precision']\n        recall = metrics[f'{label}_recall']\n        f1 = metrics[f'{label}_f1']\n        support = int(metrics[f'{label}_support'])\n        print(f\"{label:<5} {precision:<9.3f} {recall:<6.3f} {f1:<6.3f} {support}\")\n\ndef plot_confusion_matrix(cm, labels=['A', 'B', 'C', 'D'], save_path=None):\n    \"\"\"Plot confusion matrix\"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=labels, yticklabels=labels)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef analyze_answer_distribution(results_df):\n    \"\"\"Analyze the distribution of correct and predicted answers\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"ANSWER DISTRIBUTION ANALYSIS\")\n    print(\"=\" * 50)\n    \n    # True label distribution\n    true_dist = results_df['correct_label'].value_counts().sort_index()\n    print(\"\\nTrue Label Distribution:\")\n    for label in ['A', 'B', 'C', 'D']:\n        count = true_dist.get(label, 0)\n        pct = count / len(results_df) * 100\n        print(f\"  {label}: {count} ({pct:.1f}%)\")\n    \n    # Predicted label distribution (excluding None)\n    valid_preds = results_df.dropna(subset=['predicted_label'])\n    if len(valid_preds) > 0:\n        pred_dist = valid_preds['predicted_label'].value_counts().sort_index()\n        print(\"\\nPredicted Label Distribution:\")\n        for label in ['A', 'B', 'C', 'D']:\n            count = pred_dist.get(label, 0)\n            pct = count / len(valid_preds) * 100\n            print(f\"  {label}: {count} ({pct:.1f}%)\")\n    \n    # Check for bias towards certain options\n    print(\"\\nLabel Rotation Check:\")\n    if len(set(true_dist.values)) == 1:\n        print(\"✓ Perfect label rotation detected\")\n    else:\n        print(\"⚠ Uneven label distribution detected\")\n        \ndef comprehensive_qa_evaluation(results_df, save_plots=True):\n    \"\"\"Run complete evaluation pipeline\"\"\"\n    \n    # Calculate metrics\n    metrics, cm, y_true, y_pred = calculate_qa_metrics(results_df)\n    \n    if not metrics:\n        return\n    \n    # Print metrics\n    print_qa_metrics(metrics)\n    \n    # Analyze distributions\n    analyze_answer_distribution(results_df)\n    \n    # Plot confusion matrix\n    if save_plots:\n        plot_confusion_matrix(cm, save_path='confusion_matrix.png')\n    else:\n        plot_confusion_matrix(cm)\n    \n    # Print sklearn classification report for additional insights\n    valid_preds = results_df.dropna(subset=['predicted_label'])\n    if len(valid_preds) > 0:\n        print(\"\\n\" + \"=\" * 50)\n        print(\"DETAILED CLASSIFICATION REPORT\")\n        print(\"=\" * 50)\n        print(classification_report(\n            valid_preds['correct_label'], \n            valid_preds['predicted_label'],\n            labels=['A', 'B', 'C', 'D']\n        ))\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:03:14.956372Z","iopub.execute_input":"2025-10-31T00:03:14.956649Z","iopub.status.idle":"2025-10-31T00:03:15.785798Z","shell.execute_reply.started":"2025-10-31T00:03:14.956617Z","shell.execute_reply":"2025-10-31T00:03:15.785252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model in 4-bit for QLoRA\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    load_in_4bit=True,              # 4-bit quantization\n    device_map=\"auto\",              # Automatically split layers to GPU\n    bnb_4bit_quant_type=\"nf4\",      # NormalFloat4 (better for QLoRA)\n    bnb_4bit_use_double_quant=True, # More compression\n    torch_dtype=\"auto\"\n)\n\n# Prepare for LoRA fine-tuning\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=8,            # Rank\n    lora_alpha=16,  # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Wrap model with LoRA\nmodel = get_peft_model(model, lora_config)\n\nprint(\"Model ready for QLoRA fine-tuning!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:03:19.840882Z","iopub.execute_input":"2025-10-31T00:03:19.841284Z","iopub.status.idle":"2025-10-31T00:07:12.350711Z","shell.execute_reply.started":"2025-10-31T00:03:19.841263Z","shell.execute_reply":"2025-10-31T00:07:12.350008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zero_shot_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n            # More explicit prompt for single character output\n#             prompt = f\"\"\"Answer the following commonsense question.\n            \n# Question: {str(row['question'])}\n# A. {str(row['A'])}\n# B. {str(row['B'])}\n# C. {str(row['C'])}\n# D. {str(row['D'])}\n# Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n# Answer:\"\"\"\n\n            prompt = f\"\"\"Answer the following commonsense question by selecting the correct option.\n            Instructions: Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n\nQuestion: {str(row['question'])}\n\n(A) {str(row['A'])}\n(B) {str(row['B'])}\n(C) {str(row['C'])}\n(D) {str(row['D'])}\n\nAnswer:\"\"\"\n\n            # 1. TOKENIZE the input prompt string\n            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n            inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=2,\n                do_sample=False\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_token_ids = output_tokens[0][inputs['input_ids'].shape[1]:]\n            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n            \n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            \n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-30T23:54:33.667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def two_shot_and_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n\n            prompt = f\"\"\"\n                Answer the following commonsense question by selecting the correct option.\n                Instructions: Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n                \n                Example:\n                Question: Sammy wanted to go to where the people were. Where might he go?\n                        A. social venues AND quiet retreats\n                        B. local events AND social venues\n                        C. sports arenas AND quiet retreats\n                        D. sports arenas AND train platforms\n                                \n                        Answer: B\n\n                Question: The fox walked from the city into the forest, what was it looking for?\n                        (A) suitable prey AND shelter from disturbances\n                        (B) urban garden AND farm fields\n                        (C) natural habitat AND farm fields\n                        (D) suitable prey AND neighborhood pets\n                        \n                        Answer: A\n                \n                Now answer the following question:\n                \n                Question: {str(row['question'])}\n                (A) {str(row['A'])}\n                (B) {str(row['B'])}\n                (C) {str(row['C'])}\n                (D) {str(row['D'])}\n                Answer:\"\"\"\n\n\n            # 1. TOKENIZE the input prompt string\n            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n            inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=2,\n                do_sample=False\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_token_ids = output_tokens[0][inputs['input_ids'].shape[1]:]\n            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n            \n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            \n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T00:11:24.392822Z","iopub.execute_input":"2025-10-31T00:11:24.393424Z","iopub.status.idle":"2025-10-31T00:11:24.401667Z","shell.execute_reply.started":"2025-10-31T00:11:24.393402Z","shell.execute_reply":"2025-10-31T00:11:24.400939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_df = pd.read_csv('/kaggle/input/csqa-pairs-baselines/QA_AND.csv')\nresults = two_shot_and_qa(model, tokenizer, qa_df)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_twoshot_AND_results.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T22:23:02.464890Z","iopub.execute_input":"2025-10-30T22:23:02.465578Z","iopub.status.idle":"2025-10-30T22:23:20.327090Z","shell.execute_reply.started":"2025-10-30T22:23:02.465544Z","shell.execute_reply":"2025-10-30T22:23:20.326178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def two_shot_or_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n\n            prompt = f\"\"\"\n                Answer the following commonsense question by selecting the correct option.\n                Instructions: Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n                \n                Example:\n                Question: Sammy wanted to go to where the people were. Where might he go?\n                A. quiet retreats OR empty parks\n                B. local events OR empty parks\n                C. sports arenas OR empty parks\n                D. train platforms OR empty parks\n                \n                Answer: B\n\n                Question: The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\n                A. living room OR dining area\n                B. utility drawer OR kitchen counter\n                C. living room OR utility drawer\n                D. cold storage OR utility drawer\n                \n                Answer: D\n                \n                Now answer the following question:\n                \n                Question: {str(row['question'])}\n                (A) {str(row['A'])}\n                (B) {str(row['B'])}\n                (C) {str(row['C'])}\n                (D) {str(row['D'])}\n                Answer:\"\"\"\n\n\n            # 1. TOKENIZE the input prompt string\n            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n            inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=2,\n                do_sample=False\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_token_ids = output_tokens[0][inputs['input_ids'].shape[1]:]\n            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n            \n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            \n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_df = pd.read_csv('/kaggle/input/csqa-pairs-baselines/QA_OR.csv')\nresults = two_shot_or_qa(model, tokenizer, qa_df)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_twoshot_OR_results.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T22:29:58.499905Z","iopub.execute_input":"2025-10-30T22:29:58.500188Z","iopub.status.idle":"2025-10-30T22:30:16.198292Z","shell.execute_reply.started":"2025-10-30T22:29:58.500167Z","shell.execute_reply":"2025-10-30T22:30:16.197437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def two_shot_nnor_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n\n            prompt = f\"\"\"\n                Answer the following commonsense question by selecting the correct option.\n                Instructions: Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n                \n                Examples:\n                Question: Sammy wanted to go to where the people were. Where might he go? \n                A. NEITHER local events NOR train platforms \n                B. NEITHER sports arenas NOR train platforms \n                C. NEITHER social venues NOR sports arenas \n                D. NEITHER social venues NOR quiet retreats\n                \n                Answer: B\n\n                Question: The forgotten leftovers had gotten quite old, he found it covered in mold in the back of his what?\n                A. NEITHER cold storage NOR dining area\n                B. NEITHER sealed container NOR living room\n                C. NEITHER food cabinet NOR utility drawer\n                D. NEITHER living room NOR utility drawer\n                \n                Answer: D\n                \n                Now answer the following question:\n                \n                Question: {str(row['question'])}\n                (A) {str(row['A'])}\n                (B) {str(row['B'])}\n                (C) {str(row['C'])}\n                (D) {str(row['D'])}\n                Answer:\"\"\"\n\n\n            # 1. TOKENIZE the input prompt string\n            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n            inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=2,\n                do_sample=False\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_token_ids = output_tokens[0][inputs['input_ids'].shape[1]:]\n            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n            \n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            \n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_df = pd.read_csv('/kaggle/input/csqa-pairs-baselines/QA_NEITHER.csv')\nresults = two_shot_nnor_qa(model, tokenizer, qa_df)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_twoshot_NEITHER_results.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T22:30:16.199652Z","iopub.execute_input":"2025-10-30T22:30:16.199889Z","iopub.status.idle":"2025-10-30T22:30:34.262927Z","shell.execute_reply.started":"2025-10-30T22:30:16.199871Z","shell.execute_reply":"2025-10-30T22:30:34.262074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def two_shot_mixed_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n\n            prompt = f\"\"\"\n                Answer the following commonsense question by selecting the correct option.\n                Instructions: Please respond with ONLY the single, capital letter (A, B, C, or D) that is the correct answer. Do not include any other text, punctuation, or explanation.\n                \n                Examples:\n                   Question: The fox walked from the city into the forest, what was it looking for? \n        \n                    A. natural habitat AND shelter from disturbances\n                    B. farm fields OR neighborhood pets\n                    C. city park AND neighborhood pets\n                    D. farm fields AND neighborhood pets\n                        \n                Answer: A\n\n                Question: Google Maps and other highway and street GPS services have replaced what?\n                    A. historical navigation logs AND tourist information centers\n                    B. NEITHER printed road maps NOR route planning software\n                    C. manual navigation techniques AND tourist information centers\n                    D. manual navigation techniques AND route planning software\n                    \n                    Answer: D\n                \n                Now answer the following question:\n                \n                Question: {str(row['question'])}\n                (A) {str(row['A'])}\n                (B) {str(row['B'])}\n                (C) {str(row['C'])}\n                (D) {str(row['D'])}\n                Answer:\"\"\"\n\n\n            # 1. TOKENIZE the input prompt string\n            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n            inputs = {key: value.to(model.device) for key, value in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=2,\n                do_sample=False\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_token_ids = output_tokens[0][inputs['input_ids'].shape[1]:]\n            generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n            \n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            \n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_df = pd.read_csv('/kaggle/input/csqa-pairs-baselines/QA_Mixed.csv')\nresults = two_shot_mixed_qa(model, tokenizer, qa_df)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_twoshot_MIXED_results.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T22:30:34.263658Z","iopub.execute_input":"2025-10-30T22:30:34.263948Z","iopub.status.idle":"2025-10-30T22:30:52.281261Z","shell.execute_reply.started":"2025-10-30T22:30:34.263929Z","shell.execute_reply":"2025-10-30T22:30:52.280588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}