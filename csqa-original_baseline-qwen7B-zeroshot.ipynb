{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13674762,"sourceType":"datasetVersion","datasetId":8072060},{"sourceId":13674779,"sourceType":"datasetVersion","datasetId":8695264},{"sourceId":13735887,"sourceType":"datasetVersion","datasetId":8229833}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:13:28.859419Z","iopub.execute_input":"2025-11-24T10:13:28.859881Z","iopub.status.idle":"2025-11-24T10:13:28.878489Z","shell.execute_reply.started":"2025-11-24T10:13:28.859855Z","shell.execute_reply":"2025-11-24T10:13:28.877777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:13:31.750053Z","iopub.execute_input":"2025-11-24T10:13:31.750323Z","iopub.status.idle":"2025-11-24T10:13:31.754095Z","shell.execute_reply.started":"2025-11-24T10:13:31.750302Z","shell.execute_reply":"2025-11-24T10:13:31.753369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes peft trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:13:40.182309Z","iopub.execute_input":"2025-11-24T10:13:40.182970Z","iopub.status.idle":"2025-11-24T10:15:07.206386Z","shell.execute_reply.started":"2025-11-24T10:13:40.182942Z","shell.execute_reply":"2025-11-24T10:15:07.205557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"hugging_face\")\n\nfrom huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:15:07.208071Z","iopub.execute_input":"2025-11-24T10:15:07.208379Z","iopub.status.idle":"2025-11-24T10:15:07.333720Z","shell.execute_reply.started":"2025-11-24T10:15:07.208353Z","shell.execute_reply":"2025-11-24T10:15:07.332954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom tqdm import tqdm\nimport json\n\ndef load_and_filter_csqa_data():\n    \"\"\"\n    Loads the CommonsenseQA dataset and returns first 5000 rows.\n    \"\"\"\n    print(\"Loading CommonsenseQA dataset...\")\n    ds = load_dataset(\"tau/commonsense_qa\")\n    train_df = ds['train'].to_pandas()\n    valid_df = ds['validation'].to_pandas()\n    test_df = ds['test'].to_pandas()\n    df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n    print(f\"Total dataset size: {df.shape}\")\n    \n    # Take first 5000 rows\n    df_subset = df.iloc[:5000].copy()\n    df_subset.dropna(inplace=True)\n    df_subset = df_subset.drop(index=537).reset_index(drop=True)\n    print(f\"Using {len(df_subset)} questions.\")\n    return df_subset\n\ndef flatten_row(row):\n    # Extract choices\n    labels = row['choices']['label']       # ['A','B','C','D','E']\n    texts  = row['choices']['text']        # actual text\n    \n    choice_map = {label: text for label, text in zip(labels, texts)}\n    \n    correct_label = row['answerKey']\n    correct_answer_text = choice_map.get(correct_label, None)\n    \n    return {\n        'question': row['question'],\n        'A': choice_map.get('A', None),\n        'B': choice_map.get('B', None),\n        'C': choice_map.get('C', None),\n        'D': choice_map.get('D', None),\n        'E': choice_map.get('E', None),\n        'correct_label': correct_label,\n        'correct_answer_text': correct_answer_text\n    }\n\n\ndef split_dataset(df):\n    test_df = df.sample(n=500, random_state=72)\n    remaining_df = df.drop(test_df.index)\n    # Step 3. Now it's safe to reset indices for convenience\n    test_df = test_df.reset_index(drop=True)\n    remaining_df = remaining_df.reset_index(drop=True)\n    \n    # Step 4. Split remaining into dev and train\n    dev_df = remaining_df.sample(n=1500, random_state=72)\n    train_df = remaining_df.drop(dev_df.index)\n    \n    # Optional: reset indexes\n    dev_df = dev_df.reset_index(drop=True)\n    train_df = train_df.reset_index(drop=True)\n\n    return train_df.apply(flatten_row, axis=1, result_type='expand'), dev_df.apply(flatten_row, axis=1, result_type='expand'), test_df.apply(flatten_row, axis=1, result_type='expand')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:16:45.041306Z","iopub.execute_input":"2025-11-24T10:16:45.041657Z","iopub.status.idle":"2025-11-24T10:16:50.240258Z","shell.execute_reply.started":"2025-11-24T10:16:45.041632Z","shell.execute_reply":"2025-11-24T10:16:50.239421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = load_and_filter_csqa_data()\n\ntrain, dev, test = split_dataset(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:16:50.241416Z","iopub.execute_input":"2025-11-24T10:16:50.242140Z","iopub.status.idle":"2025-11-24T10:16:55.473503Z","shell.execute_reply.started":"2025-11-24T10:16:50.242117Z","shell.execute_reply":"2025-11-24T10:16:55.472933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model in 4-bit for QLoRA\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    load_in_4bit=True,              # 4-bit quantization\n    device_map=\"auto\",              # Automatically split layers to GPU\n    bnb_4bit_quant_type=\"nf4\",      # NormalFloat4 (better for QLoRA)\n    bnb_4bit_use_double_quant=True, # More compression\n    torch_dtype=\"auto\"\n)\n\n# Prepare for LoRA fine-tuning\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=8,            # Rank\n    lora_alpha=16,  # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Wrap model with LoRA\nmodel = get_peft_model(model, lora_config)\n\nprint(\"Model ready for QLoRA fine-tuning!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:29:02.182723Z","iopub.execute_input":"2025-11-24T10:29:02.183066Z","iopub.status.idle":"2025-11-24T10:30:46.162176Z","shell.execute_reply.started":"2025-11-24T10:29:02.183041Z","shell.execute_reply":"2025-11-24T10:30:46.161329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef calculate_qa_metrics(results_df):\n    \"\"\"\n    \n    Calculate comprehensive evaluation metrics for QA results\n    \n    Args:\n        results_df: DataFrame with 'predicted_label' and 'correct_label' columns\n    \n    Returns:\n        dict: Dictionary containing all evaluation metrics\n    \"\"\"\n    \n    # Remove None predictions for accurate calculation\n    valid_predictions = results_df.dropna(subset=['predicted_label'])\n    \n    if len(valid_predictions) == 0:\n        print(\"Warning: No valid predictions found!\")\n        return {}\n    \n    y_true = valid_predictions['correct_label'].tolist()\n    y_pred = valid_predictions['predicted_label'].tolist()\n    \n    # Basic metrics\n    total_questions = len(results_df)\n    answered_questions = len(valid_predictions)\n    unanswered_questions = total_questions - answered_questions\n    \n    # Accuracy (main metric for QA)\n    accuracy = accuracy_score(y_true, y_pred)\n    \n    # Per-class metrics\n    labels = ['A', 'B', 'C', 'D', 'E']\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n    \n    # Macro and micro averages\n    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='macro', zero_division=0\n    )\n    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='micro', zero_division=0\n    )\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    \n    # Answer rate (percentage of questions answered)\n    answer_rate = answered_questions / total_questions\n    \n    # Compile results\n    metrics = {\n        'total_questions': total_questions,\n        'answered_questions': answered_questions,\n        'unanswered_questions': unanswered_questions,\n        'answer_rate': answer_rate,\n        'accuracy': accuracy,\n        'macro_precision': macro_precision,\n        'macro_recall': macro_recall,\n        'macro_f1': macro_f1,\n        'micro_precision': micro_precision,\n        'micro_recall': micro_recall,\n        'micro_f1': micro_f1,\n    }\n    \n    # Per-class metrics\n    for i, label in enumerate(labels):\n        metrics[f'{label}_precision'] = precision[i]\n        metrics[f'{label}_recall'] = recall[i]\n        metrics[f'{label}_f1'] = f1[i]\n        metrics[f'{label}_support'] = support[i]\n    \n    return metrics, cm, y_true, y_pred\n\ndef print_qa_metrics(metrics):\n    \"\"\"Pretty print the evaluation metrics\"\"\"\n    print(\"=\" * 50)\n    print(\"QA EVALUATION METRICS\")\n    print(\"=\" * 50)\n    \n    print(f\"Total Questions: {metrics['total_questions']}\")\n    print(f\"Answered Questions: {metrics['answered_questions']}\")\n    print(f\"Answer Rate: {metrics['answer_rate']:.1%}\")\n    print(f\"Accuracy: {metrics['accuracy']:.1%}\")\n    \n    print(\"\\n\" + \"-\" * 30)\n    print(\"OVERALL METRICS\")\n    print(\"-\" * 30)\n    print(f\"Macro Precision: {metrics['macro_precision']:.3f}\")\n    print(f\"Macro Recall: {metrics['macro_recall']:.3f}\")\n    print(f\"Macro F1: {metrics['macro_f1']:.3f}\")\n    print(f\"Micro Precision: {metrics['micro_precision']:.3f}\")\n    print(f\"Micro Recall: {metrics['micro_recall']:.3f}\")\n    print(f\"Micro F1: {metrics['micro_f1']:.3f}\")\n    \n    print(\"\\n\" + \"-\" * 40)\n    print(\"PER-CLASS METRICS\")\n    print(\"-\" * 40)\n    print(f\"{'Class':<5} {'Precision':<9} {'Recall':<6} {'F1':<6} {'Support'}\")\n    print(\"-\" * 40)\n    \n    for label in ['A', 'B', 'C', 'D', 'E']:\n        precision = metrics[f'{label}_precision']\n        recall = metrics[f'{label}_recall']\n        f1 = metrics[f'{label}_f1']\n        support = int(metrics[f'{label}_support'])\n        print(f\"{label:<5} {precision:<9.3f} {recall:<6.3f} {f1:<6.3f} {support}\")\n\ndef plot_confusion_matrix(cm, labels=['A', 'B', 'C', 'D', 'E'], save_path=None):\n    \"\"\"Plot confusion matrix\"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=labels, yticklabels=labels)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef analyze_answer_distribution(results_df):\n    \"\"\"Analyze the distribution of correct and predicted answers\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"ANSWER DISTRIBUTION ANALYSIS\")\n    print(\"=\" * 50)\n    \n    # True label distribution\n    true_dist = results_df['correct_label'].value_counts().sort_index()\n    print(\"\\nTrue Label Distribution:\")\n    for label in ['A', 'B', 'C', 'D','E']:\n        count = true_dist.get(label, 0)\n        pct = count / len(results_df) * 100\n        print(f\"  {label}: {count} ({pct:.1f}%)\")\n    \n    # Predicted label distribution (excluding None)\n    valid_preds = results_df.dropna(subset=['predicted_label'])\n    if len(valid_preds) > 0:\n        pred_dist = valid_preds['predicted_label'].value_counts().sort_index()\n        print(\"\\nPredicted Label Distribution:\")\n        for label in ['A', 'B', 'C', 'D','E']:\n            count = pred_dist.get(label, 0)\n            pct = count / len(valid_preds) * 100\n            print(f\"  {label}: {count} ({pct:.1f}%)\")\n    \n    # Check for bias towards certain options\n    print(\"\\nLabel Rotation Check:\")\n    if len(set(true_dist.values)) == 1:\n        print(\"✓ Perfect label rotation detected\")\n    else:\n        print(\"⚠ Uneven label distribution detected\")\n        \ndef comprehensive_qa_evaluation(results_df, save_plots=True):\n    \"\"\"Run complete evaluation pipeline\"\"\"\n    \n    # Calculate metrics\n    metrics, cm, y_true, y_pred = calculate_qa_metrics(results_df)\n    \n    if not metrics:\n        return\n    \n    # Print metrics\n    print_qa_metrics(metrics)\n    \n    # Analyze distributions\n    analyze_answer_distribution(results_df)\n    \n    # Plot confusion matrix\n    if save_plots:\n        plot_confusion_matrix(cm, save_path='confusion_matrix.png')\n    else:\n        plot_confusion_matrix(cm)\n    \n    # Print sklearn classification report for additional insights\n    valid_preds = results_df.dropna(subset=['predicted_label'])\n    if len(valid_preds) > 0:\n        print(\"\\n\" + \"=\" * 50)\n        print(\"DETAILED CLASSIFICATION REPORT\")\n        print(\"=\" * 50)\n        print(classification_report(\n            valid_preds['correct_label'], \n            valid_preds['predicted_label'],\n            labels=['A', 'B', 'C', 'D','E']\n        ))\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:19:01.392277Z","iopub.execute_input":"2025-11-24T10:19:01.392892Z","iopub.status.idle":"2025-11-24T10:19:01.617005Z","shell.execute_reply.started":"2025-11-24T10:19:01.392871Z","shell.execute_reply":"2025-11-24T10:19:01.616131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zero_shot_qa(model, tokenizer, qa_dataframe):\n    results = []\n    required_cols = ['question', 'A', 'B', 'C', 'D', 'E', 'correct_label']\n    if not all(col in qa_dataframe.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain the following columns: {required_cols}\")\n    \n    valid_choices = {'A', 'B', 'C', 'D', 'E'}\n    \n    for idx, row in qa_dataframe.iterrows():\n        predicted_label = None\n        try:\n\n            prompt = f\"\"\"Answer the following commonsense question by selecting the correct option.\n            Instructions: Please respond with ONLY the single, capital letter (A, B, C, D or E) that is the correct answer. Do not include any other text, punctuation, or explanation.\n\nQuestion: {str(row['question'])}\n\n(A) {str(row['A'])}\n(B) {str(row['B'])}\n(C) {str(row['C'])}\n(D) {str(row['D'])}\n(E) {str(row['E'])}\nAnswer:\"\"\"\n\n            conversation = [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the following commonsense question by selecting the correct option.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n\n            # 1. TOKENIZE the input prompt string\n            tokenized_inputs = tokenizer.apply_chat_template(\n                conversation,\n                add_generation_prompt=True,\n                return_tensors=\"pt\"\n            )\n\n            if isinstance(tokenized_inputs, torch.Tensor):\n                inputs = {\"input_ids\": tokenized_inputs, \"attention_mask\": torch.ones_like(tokenized_inputs)}\n            else:\n                inputs = tokenized_inputs\n            \n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n            \n            # 2. GENERATE the response using the tokenized input\n            # We explicitly pass `input_ids` and `attention_mask`\n            output_tokens = model.generate(\n                **inputs,\n                max_new_tokens=3,  # just one letter\n                do_sample=False,\n                temperature=0.0,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            \n            # 3. DECODE the output tokens back into a string\n            # We slice the output to get only the newly generated tokens\n            generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n\n\n            if \"Answer:\" in generated_text:\n                generated_text = generated_text.split(\"Answer:\")[-1]\n            # print(generated_text)\n            # Clean up extra prefixes or artifacts\n            generated_text = generated_text.strip()  # remove spaces/newlines\n            if generated_text.lower().startswith(\"assistant\"):\n                generated_text = generated_text.splitlines()[-1].strip()  # keep the last meaningful line\n\n            # 4. Process the generated text for a strict answer\n            # Strip all whitespace and common punctuation, then get the first character\n            cleaned_text = generated_text.strip().upper().replace('.', '').replace(',', '').replace(' ', '')\n            # print(cleaned_text)\n            # Validate that the first character is a valid choice\n            if cleaned_text and cleaned_text[0] in valid_choices:\n                predicted_label = cleaned_text[0]\n\n        except Exception as e:\n            # Catch any errors during processing and log them\n            print(f\"Error processing row {idx}: {e}\")\n        \n        results.append({\n            'question': str(row['question']),\n            'predicted_label': predicted_label,\n            'correct_label': str(row['correct_label']).strip().upper(),\n            'full_qa': f\"\"\"Question: {str(row['question'])} \\n\n            A. {str(row['A'])}\n            B. {str(row['B'])}\n            C. {str(row['C'])}\n            D. {str(row['D'])}\n            E. {str(row['E'])}\"\"\"\n        })\n            \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:33:23.702072Z","iopub.execute_input":"2025-11-24T10:33:23.702887Z","iopub.status.idle":"2025-11-24T10:33:23.716267Z","shell.execute_reply.started":"2025-11-24T10:33:23.702852Z","shell.execute_reply":"2025-11-24T10:33:23.715194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = zero_shot_qa(model, tokenizer, train)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_zeroshot_baseline_results_train.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:33:28.559830Z","iopub.execute_input":"2025-11-24T10:33:28.560131Z","iopub.status.idle":"2025-11-24T10:33:41.100235Z","shell.execute_reply.started":"2025-11-24T10:33:28.560111Z","shell.execute_reply":"2025-11-24T10:33:41.099368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = zero_shot_qa(model, tokenizer, dev)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_zeroshot_baseline_results_dev.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:21:20.615858Z","iopub.execute_input":"2025-11-24T10:21:20.616207Z","iopub.status.idle":"2025-11-24T10:21:32.307504Z","shell.execute_reply.started":"2025-11-24T10:21:20.616184Z","shell.execute_reply":"2025-11-24T10:21:32.306838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = zero_shot_qa(model, tokenizer, test)\n\ny_results = pd.DataFrame(results)\ny_results.to_csv('csqa_zeroshot_baseline_results_test.csv',index=False)\n\n\n# Run comprehensive evaluation\nmetrics = calculate_qa_metrics(y_results)\nprint(metrics)\n\nfor res in results[:20]:\n    print(f\"Q: {res['full_qa']}\")\n    print(f\"Predicted answer: {res['predicted_label']}\")\n    print(f\"Correct answer: {res['correct_label']}\")\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:21:32.308527Z","iopub.execute_input":"2025-11-24T10:21:32.308833Z","iopub.status.idle":"2025-11-24T10:21:46.906839Z","shell.execute_reply.started":"2025-11-24T10:21:32.308806Z","shell.execute_reply":"2025-11-24T10:21:46.906007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}